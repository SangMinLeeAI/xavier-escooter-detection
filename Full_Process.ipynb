{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e1867f7-0994-4da8-828b-fa4169f380a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch import Tensor\n",
    "# model\n",
    "from torchvision.models import efficientnet_v2_m, EfficientNet_V2_M_Weights\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "##dataset\n",
    "\n",
    "# other\n",
    "import IPython.display as display\n",
    "import numpy as np\n",
    "from typing import Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "184dbdaf-2e93-4bb2-a8cc-28ad2b3417ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## person model\n",
    "\n",
    "person_weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "person_model = fasterrcnn_resnet50_fpn_v2(weights=person_weights, box_score_thresh=0.9)\n",
    "person_model.eval()\n",
    "\n",
    "# Step 2: Initialize the inference transforms\n",
    "person_preprocess = person_weights.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14a28270-5402-4041-a451-6d941db556ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=RGB size=1058x466 at 0x2B4D1FF74220>\n"
     ]
    }
   ],
   "source": [
    "## scooter model\n",
    "scooter_weight_path = \"./scooter_model.pth\"\n",
    "scooter_weights = EfficientNet_V2_M_Weights.DEFAULT\n",
    "scooter_preprocess = scooter_weights.transforms()\n",
    "scooter_model = efficientnet_v2_m(weights=scooter_weights)\n",
    "scooter_model.classifier[-1] = nn.Linear(1280, 1)\n",
    "scooter_model = nn.DataParallel(scooter_model, device_ids=[0, 1, 2, 3])\n",
    "scooter_model.load_state_dict(torch.load(scooter_weight_path, map_location=device))\n",
    "scooter_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b98cc75-682e-44f5-9ec3-cfc16e72bbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "## helmet model\n",
    "helmet_weight_path = \"./helmet_model.pth\"\n",
    "helmet_weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "helmet_preprocess = helmet_weights.transforms()\n",
    "helmet_model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(weights=helmet_weights, box_score_thresh=0.9)\n",
    "in_features = helmet_model.roi_heads.box_predictor.cls_score.in_features\n",
    "helmet_model.roi_heads.box_predictor = FastRCNNPredictor(in_features, 8)\n",
    "helmet_model.load_state_dict(torch.load(helmet_weight_path, map_location=device))\n",
    "helmet_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def resize_box(box: Tensor, factor: float = 0.3) -> Tensor:\n",
    "    \"\"\"Resize the box coordinates.\"\"\"\n",
    "    box[0] -= int((box[2] - box[0]) * factor)\n",
    "    box[1] -= int((box[3] - box[1]) * factor)\n",
    "    box[2] += int((box[2] - box[0]) * factor)\n",
    "    box[3] += int((box[3] - box[1]) * factor)\n",
    "    return box\n",
    "\n",
    "\n",
    "def crop_and_transform(image: Tensor, box: Tensor, device: str) -> Tensor:\n",
    "    \"\"\"Crop the image using the given box coordinates and transform it to a tensor.\"\"\"\n",
    "    box_image = torchvision.transforms.functional.crop(image, box[1], box[0], box[3] - box[1], box[2] - box[0])\n",
    "    return box_image\n",
    "\n",
    "\n",
    "def change_image_for_scooter(image_tensor: Tensor) -> Tensor:\n",
    "    \"\"\"Resize the image tensor for scooter model.\"\"\"\n",
    "    preprocess_image = scooter_preprocess(image_tensor).squeeze(0).to(device)\n",
    "    return preprocess_image\n",
    "\n",
    "\n",
    "def is_person_valid(label: int) -> bool:\n",
    "    \"\"\"Check if the label corresponds to a valid person.\"\"\"\n",
    "    return label == 1\n",
    "\n",
    "\n",
    "def change_image_for_helmet(image_tensor: Tensor) -> Tensor:\n",
    "    \"\"\"Resize the image tensor for helmet model.\"\"\"\n",
    "    preprocess_image = helmet_preprocess(image_tensor).squeeze(0).to(device)\n",
    "    return preprocess_image\n",
    "\n",
    "\n",
    "def is_helmet_present(model, image_tensor: Tensor) -> bool:\n",
    "    \"\"\"Check if a helmet is present in the image.\"\"\"\n",
    "    result = model(change_image_for_helmet(image_tensor))\n",
    "    helmet_labels = result[0][\"labels\"].tolist()\n",
    "    return 1 in helmet_labels or 2 in helmet_labels or 3 in helmet_labels\n",
    "\n",
    "\n",
    "def is_scooter_present(model, image_tensor: Tensor) -> bool:\n",
    "    \"\"\"Check if a scooter is present in the image.\"\"\"\n",
    "    result = model(change_image_for_scooter(image_tensor))\n",
    "    predicted = (torch.sigmoid(result) > 0.2).float().item()\n",
    "    return predicted == 1\n",
    "\n",
    "\n",
    "def inference(\n",
    "        helmet_model,\n",
    "        person_model,\n",
    "        scooter_model,\n",
    "        image: Tensor,\n",
    "        device: str = \"cpu\",\n",
    ") -> Tuple[List[np.ndarray], List[int], List[float]]:\n",
    "    batch = [person_preprocess(image)]\n",
    "    person_result = person_model(batch)[0]\n",
    "\n",
    "    boxes: Tensor = person_result[0][\"boxes\"]\n",
    "    labels: Tensor = person_result[0][\"labels\"]\n",
    "    scores: Tensor = person_result[0][\"scores\"]\n",
    "\n",
    "    valid_indices: List[int] = [\n",
    "        index\n",
    "        for index, (label, score) in enumerate(zip(labels, scores))\n",
    "        if is_person_valid(label)\n",
    "    ]\n",
    "\n",
    "    for index in valid_indices:\n",
    "        box = resize_box(boxes[index])\n",
    "        box_image_tensor = crop_and_transform(image, box, device)\n",
    "\n",
    "        if is_scooter_present(scooter_model, box_image_tensor):\n",
    "            if is_helmet_present(helmet_model, box_image_tensor):\n",
    "                labels[index] = 2\n",
    "            else:\n",
    "                labels[index] = 3\n",
    "    return (\n",
    "        [boxes[i] for i in valid_indices],\n",
    "        [labels[i] for i in valid_indices],\n",
    "        [scores[i] for i in valid_indices],\n",
    "    )\n",
    "\n",
    "\n",
    "def draw_box(\n",
    "        img: Tensor,\n",
    "        boxes: List[Tensor],\n",
    "        labels: List[str],\n",
    "):\n",
    "    box = draw_bounding_boxes(img, boxes=boxes,\n",
    "                              labels=labels,\n",
    "                              colors=\"red\",\n",
    "                              fill=True,\n",
    "                              width=4, font_size=40)\n",
    "    im = to_pil_image(box.detach())\n",
    "    display.display(im)\n",
    "num_frames = 100\n",
    "video_path = \"./test_video.mp4\"\n",
    "output_path = \"./result_video.mp4\"\n",
    "video = torchvision.io.read_video(video_path, start_pts=0, end_pts=num_frames)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c19d280220804ed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result_video = []\n",
    "for frame_idx in range(num_frames):\n",
    "    frame = video[frame_idx].to(device)\n",
    "    result = inference(helmet_model,\n",
    "                       person_model=person_model,\n",
    "                       scooter_model=scooter_model,\n",
    "                       image=frame,\n",
    "                       device=device)\n",
    "    if len(result[0]) == 0:\n",
    "        continue\n",
    "    d_box = draw_box(frame, boxes=torch.stack(result[0]), labels=result[1])\n",
    "    result_video.append(d_box.permute(1, 2, 0))\n",
    "\n",
    "torchvision.iowrite_video(output_path, result_video, info['video_fps'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc87634771fa9e68"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sangmin_env",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
